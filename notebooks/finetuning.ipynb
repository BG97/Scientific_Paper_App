{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5541c68-42b1-4a18-8587-2739632076cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing all required libraries for environment management, data processing, model integration, and API use.\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26072b1e-8cab-4207-ae7c-66cbf5894cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Hugging Face access token from environment and authenticate to the Hugging Face Hub.\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['HUGGINGFACE_RW_TOKEN'] = os.getenv('HUGGINGFACE_RW_TOKEN', 'your-key-if-not-using-env')\n",
    "\n",
    "hf_token = os.environ['HUGGINGFACE_RW_TOKEN']\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7639878b-0807-4617-b104-9465173ec8b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load a subset of the scientific papers dataset (train and test) from Hugging Face Datasets.\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"scientific_papers\", \"arxiv\", split=\"train[:1000]\", trust_remote_code=True)\n",
    "test = load_dataset(\"scientific_papers\", \"arxiv\", split=\"test[:10]\", trust_remote_code=True)\n",
    "print(test[0]['article'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9190780f-6939-453b-b4ad-1516a5c893d8",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to clean and normalize scientific text using regex patterns.\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "\n",
    "    text = re.sub(r'\\$+', '', text)\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\b', '', text) \n",
    "\n",
    "    text = re.sub(r'\\[[^\\]]{1,30}\\]', '', text)  \n",
    "\n",
    "    text = re.sub(r'[\\*\\?Â·â€¥â€¦â€§]+', '', text)\n",
    "\n",
    "    text = re.sub(r'[,.]{2,}', '.', text)\n",
    "\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96a9195-bf07-412b-b981-ef71a4832413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleaning function to all samples in the dataset and store cleaned articles and abstracts.\n",
    "\n",
    "from tqdm import tqdm\n",
    "cleaned_dataset = []\n",
    "for sample in tqdm(dataset): \n",
    "    cleaned_sample = {\n",
    "        \"article\": clean_text(sample[\"article\"]),\n",
    "        \"abstract\": clean_text(sample[\"abstract\"])\n",
    "    }\n",
    "    cleaned_dataset.append(cleaned_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3eb46f-ec19-4569-ba91-d1463e406858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate instruction-style prompt-completion pairs from cleaned dataset and save to JSONL for fine-tuning.\n",
    "\n",
    "import random\n",
    "output_dir = \"data\"\n",
    "output_path = os.path.join(output_dir, \"scipaper_instruct_100.jsonl\")\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for i in tqdm(range(100)):\n",
    "        article = cleaned_dataset[i]['article']\n",
    "        abstract = cleaned_dataset[i]['abstract']\n",
    "\n",
    "\n",
    "\n",
    "        # summary\n",
    "        prompt = f\"\"\"Instruction:\n",
    "Summarize the following article.\n",
    "\n",
    "Article:\n",
    "{article}\n",
    "\n",
    "### Response:\"\"\"\n",
    "        completion = f\" {abstract.strip()}\" \n",
    "        \n",
    "        f.write(json.dumps({\n",
    "            \"prompt\": prompt,\n",
    "            \"completion\": completion\n",
    "        }) + \"\\n\")\n",
    "\n",
    "print(f\"Saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0942ceaa-e4b3-4edd-87f4-c063fa40b4b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preview the first few entries in the generated JSONL file to verify formatting and content.\n",
    "\n",
    "with open(\"data/scipaper_instruct_1000.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for i in range(3):\n",
    "        print(json.loads(f.readline()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8d4d3d-938f-483b-b68f-ffbae2841220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model names, user ID, and run name with timestamp for logging and saving to Hugging Face hub.\n",
    "\n",
    "from datetime import datetime\n",
    "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "PROJECT_NAME = \"ScientificPaperLLMs\"\n",
    "HF_USER = \"Benny97\"\n",
    "# Run name for saving the model in the hub\n",
    "RUN_NAME =  f\"{datetime.now():%Y-%m-%d_%H.%M.%S}\"\n",
    "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
    "HUB_MODEL_NAME = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
    "\n",
    "QUANT_4_BIT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a927a510-7f45-4be7-8343-fefd974b1fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for dataset loading, model configuration, and PEFT training.\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig,set_seed\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62d497e-e8f9-4d5d-a0c0-81b9a3031e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure quantization settings (4-bit or 8-bit) based on QUANT_4_BIT flag for efficient model loading.\n",
    "\n",
    "if QUANT_4_BIT:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    "  )\n",
    "else:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d7a7aa-4cb2-453d-9238-235fa2c6fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and base model with quantization, and report memory usage after loading the model.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=quant_config,\n",
    "    device_map='cuda'\n",
    ")\n",
    "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf88b5-5228-4934-8d87-8bc353b3218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data collator to identify the response section during training using the response template.\n",
    "\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template=\"### Response:\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe9ec5a-eb81-4a77-9e65-a7e1e0a6f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and filter out samples whose tokenized prompt + completion exceed the max token limit.\n",
    "\n",
    "max_len = 3000\n",
    "data_path = \"data/scipaper_instruct_1000.jsonl\"\n",
    "dataset = load_dataset(\"json\", data_files=data_path)[\"train\"]\n",
    "def filter_fn(example):\n",
    "    merged = example[\"prompt\"] + example[\"completion\"]\n",
    "    return len(tokenizer(merged)[\"input_ids\"]) <= max_len\n",
    "\n",
    "filtered_dataset = dataset.filter(filter_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e35d30-a798-400d-ab3f-2adfa7ea8235",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Configure LoRA, training hyperparameters, and initialize the SFTTrainer for fine-tuning the model.\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=PROJECT_RUN_NAME,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type='cosine',\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    save_total_limit=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to=\"none\",\n",
    "    max_seq_length=12000,         \n",
    "    gradient_checkpointing=True,\n",
    "    dataset_text_field=\"text\",\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "fine_tuning = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=filtered_dataset,\n",
    "    peft_config=lora_config,\n",
    "    args=training_args,\n",
    "    data_collator=collator,\n",
    "    \n",
    "  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b3656b-edc2-49f1-8658-4710285dcbf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start the fine-tuning process using the configured model and training arguments.\n",
    "\n",
    "fine_tuning.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e6992e-7da5-47b7-ad67-fca70efd49f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the fine-tuned adapter model to your Hugging Face Hub repository.\n",
    "\n",
    "fine_tuning.model.push_to_hub(PROJECT_RUN_NAME, private=True)\n",
    "print(f\"Saved to the hub: {PROJECT_RUN_NAME}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011f43be-5a5e-46c9-989a-223ca6b5b0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define naming variables for reloading the uploaded fine-tuned model.\n",
    "\n",
    "RUN_NAME = \"2025-05-10_22.43.03\"\n",
    "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
    "REVISION = None\n",
    "FINETUNED_MODEL = f\"{HF_USER}/{PROJECT_RUN_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739b735f-fec6-45af-8f7e-55a3aeadedc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned adapter model from the Hugging Face Hub into memory.\n",
    "\n",
    "from peft import PeftModel\n",
    "fine_tuned_model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef92f2e-6935-4eaa-be64-3f49d02911cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate model output based on a prompt using the fine-tuned model.\n",
    "\n",
    "def model_Gen(prompt):\n",
    "    set_seed(42)\n",
    "\n",
    "    if \"### Response:\" in prompt:\n",
    "        prompt = prompt.split(\"### Response:\")[0] + \"### Response:\"\n",
    "\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = fine_tuned_model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "\n",
    "    generated_tokens = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fddfd3-1955-40c2-98c8-32c8d89f5832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a formatted prompt (summary or Q&A) and call the model to generate a response.\n",
    "\n",
    "def generate_response(article, user_message, mode):\n",
    "    if not article:\n",
    "        return \"Please upload or input an article.\"\n",
    "\n",
    "    if mode == \"Summarize\":\n",
    "        prompt = f\"\"\"### Instruction:\n",
    "                    Summarize the following article.\n",
    "                    \n",
    "                    ### Article:\n",
    "                    {article}\n",
    "                    \n",
    "                    ### Response:\"\"\"\n",
    "    else:  # Chat mode\n",
    "        if not user_message:\n",
    "            return \"Please enter a message to chat about the article.\"\n",
    "        prompt = f\"\"\"### Instruction:\n",
    "                    You are a helpful assistant. Answer the question below based on the article.\n",
    "                    \n",
    "                    ### Article:\n",
    "                    {article}\n",
    "                    \n",
    "                    ### Question:\n",
    "                    {user_message}\n",
    "                    \n",
    "                    ### Response:\"\"\"\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(fine_tuned_model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = fine_tuned_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            do_sample=False\n",
    "        )\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    if \"### Response:\" in generated:\n",
    "        return generated.split(\"### Response:\")[-1].strip()\n",
    "    return generated.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eeb0fe-e887-4081-a72d-e4b539044065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gradio library for building the web UI.\n",
    "\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efeb0d9-4c84-4c09-87c4-095944f74e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Gradio app with upload, text input, and generate response buttons.\n",
    "\n",
    "def process_file(file):\n",
    "    with open(file.name, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "def clear_article():\n",
    "    return \"\"\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# ðŸ¤– Scientific Article Assistant\")\n",
    "    mode = gr.Radio([\"Summarize\", \"Chat\"], value=\"Summarize\", label=\"Mode\")\n",
    "    article_input = gr.Textbox(lines=15, label=\"Paste Article or Upload File\")\n",
    "    file_upload = gr.File(label=\"Or Upload a .txt File\", file_types=[\".txt\"])\n",
    "    clear_btn = gr.Button(\"ðŸ—‘ Clear Article Input\", variant=\"secondary\")\n",
    "    user_message = gr.Textbox(lines=2, label=\"Your Question (for Chat mode)\")\n",
    "    output = gr.Textbox(label=\"Model Response\")\n",
    "    generate_btn = gr.Button(\"Generate\")\n",
    "    clear_btn.click(fn=clear_article, outputs=article_input)\n",
    "    file_upload.change(fn=process_file, inputs=file_upload, outputs=article_input)\n",
    "    generate_btn.click(fn=generate_response, inputs=[article_input, user_message, mode], outputs=output)\n",
    "\n",
    "# Launch the app\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b9f2b-4181-4f7e-880a-d06812472dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scientificPaperLLM)",
   "language": "python",
   "name": "scientificpaperllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
